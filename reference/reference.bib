%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for Donald Cheung at 2018-05-24 20:19:45 +0800 


%% Saved with string encoding Unicode (UTF-8) 



@article{Bowman:2015aa,
	Abstract = {Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.
},
	Author = {Samuel R. Bowman and Gabor Angeli and Christopher Potts and Christopher D. Manning},
	Date-Added = {2018-05-24 10:56:15 +0000},
	Date-Modified = {2018-05-24 12:19:44 +0000},
	Eprint = {1508.05326},
	Journal = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	Month = {08},
	Title = {A large annotated corpus for learning natural language inference
},
	Url = {https://arxiv.org/pdf/1508.05326},
	Year = {2015},
	Bdsk-Url-1 = {https://arxiv.org/abs/1508.05326},
	Bdsk-Url-2 = {https://arxiv.org/pdf/1508.05326}}

@article{Parikh:2016aa,
	Abstract = {We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.
},
	Author = {Ankur P. Parikh and Oscar T{\"a}ckstr{\"o}m and Dipanjan Das and Jakob Uszkoreit},
	Date-Added = {2018-05-15 07:24:09 +0000},
	Date-Modified = {2018-05-15 07:24:09 +0000},
	Eprint = {1606.01933},
	Month = {06},
	Title = {A Decomposable Attention Model for Natural Language Inference},
	Url = {https://arxiv.org/pdf/1606.01933},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1606.01933},
	Bdsk-Url-2 = {https://arxiv.org/pdf/1606.01933}}

@inproceedings{juan2016field,
	Author = {Juan, Yuchin and Zhuang, Yong and Chin, Wei-Sheng and Lin, Chih-Jen},
	Booktitle = {Proceedings of the 10th ACM Conference on Recommender Systems},
	Date-Added = {2018-05-10 13:30:19 +0000},
	Date-Modified = {2018-05-10 13:32:59 +0000},
	Organization = {ACM},
	Pages = {43--50},
	Title = {Field-aware factorization machines for CTR prediction},
	Url = {https://www.andrew.cmu.edu/user/yongzhua/conferences/ffm.pdf},
	Year = {2016},
	Bdsk-Url-1 = {https://www.andrew.cmu.edu/user/yongzhua/conferences/ffm.pdf}}

@inproceedings{glorot2010understanding,
	Address = {Understanding the difficulty of training deep feedforward neural networks},
	Author = {Glorot, Xavier and Bengio, Yoshua},
	Booktitle = {Proceedings of the thirteenth international conference on artificial intelligence and statistics},
	Date-Added = {2018-04-24 12:02:27 +0000},
	Date-Modified = {2018-04-24 12:05:20 +0000},
	Pages = {249--256},
	Title = {Understanding the difficulty of training deep feedforward neural networks},
	Url = {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
	Year = {2010},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf}}

@article{Pascanu:2012aa,
	Abstract = {There are two widely known issues with properly training Recurrent Neural Networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.
},
	Author = {Razvan Pascanu and Tomas Mikolov and Yoshua Bengio},
	Date-Added = {2018-04-24 11:02:44 +0000},
	Date-Modified = {2018-04-24 11:02:44 +0000},
	Eprint = {1211.5063},
	Month = {11},
	Title = {On the difficulty of training Recurrent Neural Networks},
	Url = {https://arxiv.org/pdf/1211.5063},
	Year = {2012},
	Bdsk-Url-1 = {https://arxiv.org/abs/1211.5063},
	Bdsk-Url-2 = {https://arxiv.org/pdf/1211.5063}}

@article{Ioffe:2015aa,
	Abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.
},
	Author = {Sergey Ioffe and Christian Szegedy},
	Date-Added = {2018-04-23 06:45:02 +0000},
	Date-Modified = {2018-04-23 06:45:02 +0000},
	Eprint = {1502.03167},
	Month = {02},
	Title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
	Url = {https://arxiv.org/pdf/1502.03167},
	Year = {2015},
	Bdsk-Url-1 = {https://arxiv.org/abs/1502.03167},
	Bdsk-Url-2 = {https://arxiv.org/pdf/1502.03167}}

@inproceedings{fischer2012introduction,
	Author = {Fischer, Asja and Igel, Christian},
	Booktitle = {Iberoamerican Congress on Pattern Recognition},
	Date-Added = {2018-03-27 03:31:10 +0000},
	Date-Modified = {2018-04-17 04:08:19 +0000},
	Organization = {Springer},
	Pages = {14--36},
	Title = {An introduction to restricted Boltzmann machines},
	Url = {https://pdfs.semanticscholar.org/dd13/5a89b5075af5cbef5becaf419457cdd77cc9.pdf},
	Year = {2012},
	Bdsk-Url-1 = {https://pdfs.semanticscholar.org/dd13/5a89b5075af5cbef5becaf419457cdd77cc9.pdf}}

@article{hinton1995wake,
	Author = {Hinton, Geoffrey E and Dayan, Peter and Frey, Brendan J and Neal, Radford M},
	Date-Added = {2018-03-27 03:25:14 +0000},
	Date-Modified = {2018-03-27 03:26:36 +0000},
	Journal = {Science},
	Number = {5214},
	Pages = {1158--1161},
	Publisher = {American Association for the Advancement of Science},
	Title = {The wake-sleep algorithm for unsupervised neural networks},
	Url = {http://www.cs.toronto.edu/~fritz/absps/ws.pdf},
	Volume = {268},
	Year = {1995},
	Bdsk-Url-1 = {http://www.cs.toronto.edu/~fritz/absps/ws.pdf}}

@article{hinton2006fast,
	Author = {Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
	Date-Added = {2018-03-27 03:21:18 +0000},
	Date-Modified = {2018-03-27 03:22:25 +0000},
	Journal = {Neural computation},
	Number = {7},
	Pages = {1527--1554},
	Publisher = {MIT Press},
	Title = {A fast learning algorithm for deep belief nets},
	Url = {https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf},
	Volume = {18},
	Year = {2006},
	Bdsk-Url-1 = {https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf}}

@inproceedings{glorot2011deep,
	Author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
	Booktitle = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
	Date-Added = {2018-03-26 03:46:50 +0000},
	Date-Modified = {2018-03-26 03:49:38 +0000},
	Keywords = {ReLU},
	Pages = {315--323},
	Title = {Deep sparse rectifier neural networks},
	Url = {http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf},
	Year = {2011},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf}}

@article{Mnih:2013aa,
	Author = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
	Date-Added = {2018-03-08 11:03:23 +0000},
	Date-Modified = {2018-03-08 11:03:23 +0000},
	Eprint = {1312.5602},
	Month = {12},
	Title = {Playing Atari with Deep Reinforcement Learning},
	Url = {https://arxiv.org/abs/1312.5602},
	Year = {2013},
	Bdsk-Url-1 = {https://arxiv.org/abs/1312.5602}}

@article{Srivastava14a,
	Author = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
	Date-Added = {2018-01-31 09:35:27 +0000},
	Date-Modified = {2018-01-31 09:40:13 +0000},
	Journal = {Journal of Machine Learning Research},
	Pages = {1929-1958},
	Title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
	Url = {http://jmlr.org/papers/v15/srivastava14a.html},
	Volume = {15},
	Year = {2014},
	Bdsk-Url-1 = {http://jmlr.org/papers/v15/srivastava14a.html}}

@article{Graves:2013aa,
	Abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
	Author = {Alex Graves},
	Date-Added = {2018-01-31 09:29:40 +0000},
	Date-Modified = {2018-01-31 09:31:58 +0000},
	Eprint = {1308.0850},
	Journal = {CoRR},
	Keywords = {LSTM, Generate Model},
	Month = {08},
	Title = {Generating Sequences With Recurrent Neural Networks},
	Url = {https://arxiv.org/abs/1308.0850},
	Volume = {abs/1308.0850},
	Year = {2013},
	Bdsk-Url-1 = {https://arxiv.org/abs/1308.0850}}

@article{Zaremba:2014aa,
	Abstract = {We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.},
	Author = {Wojciech Zaremba and Ilya Sutskever and Oriol Vinyals},
	Date-Added = {2018-01-31 09:05:49 +0000},
	Date-Modified = {2018-01-31 09:08:38 +0000},
	Eprint = {1409.2329},
	Journal = {CoRR},
	Keywords = {LSTM, Dropout},
	Month = {09},
	Title = {Recurrent Neural Network Regularization},
	Url = {https://arxiv.org/abs/1409.2329},
	Volume = {abs/1409.2329},
	Year = {2014},
	Bdsk-Url-1 = {https://arxiv.org/abs/1409.2329}}

@article{Zeiler:2012aa,
	Abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
	Author = {Matthew D. Zeiler},
	Date-Added = {2018-01-18 14:04:51 +0000},
	Date-Modified = {2018-01-29 06:41:27 +0000},
	Eprint = {1212.5701},
	Journal = {CoRR},
	Keywords = {ADADELTA, Adaptive Learning Rate, SGD},
	Month = {Dec},
	Rating = {0},
	Title = {ADADELTA: An Adaptive Learning Rate Method},
	Url = {https://arxiv.org/abs/1212.5701},
	Volume = {abs/1212.5701},
	Year = {2012},
	Bdsk-Url-1 = {https://arxiv.org/abs/1212.5701}}

@article{Rabiner:HMMtutorial,
	Abstract = {This tutorial provides an overview of the basic theory of hidden Markov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and gives practical details on methods of implementation of the theory along with a description of selected applications of the theory to distinct problems in speech recognition. Results from a number of original sources are combined to provide a single source of acquiring the background required to pursue further this area of research. The author first reviews the theory of discrete Markov chains and shows how the concept of hidden states, where the observation is a probabilistic function of the state, can be used effectively. The theory is illustrated with two simple examples, namely coin-tossing, and the classic balls-in-urns system. Three fundamental problems of HMMs are noted and several practical techniques for solving these problems are given. The various types of HMMs that have been studied, including ergodic as well as left-right models, are described},
	Author = {L. R. Rabiner},
	Date-Modified = {2018-02-27 06:38:28 +0000},
	Doi = {10.1109/5.18626},
	Issn = {0018-9219},
	Journal = {Proceedings of the IEEE},
	Keywords = {Markov processes;speech recognition;balls-in-urns system;coin-tossing;discrete Markov chains;ergodic models;hidden Markov models;hidden states;left-right models;probabilistic function;speech recognition;Distortion;Hidden Markov models;Mathematical model;Multiple signal classification;Signal processing;Speech recognition;Statistical analysis;Stochastic processes;Temperature measurement;Tutorial},
	Month = {Feb},
	Number = {2},
	Pages = {257-286},
	Title = {A tutorial on hidden Markov models and selected applications in speech recognition},
	Volume = {77},
	Year = {1989},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/5.18626}}

@article{rendle:tist2012,
	Address = {New York, NY, USA},
	Articleno = {57},
	Author = {Rendle, Steffen},
	Date-Modified = {2018-05-13 08:10:10 +0000},
	Issn = {2157-6904},
	Issue_Date = {May 2012},
	Journal = {ACM Trans. Intell. Syst. Technol.},
	Month = May,
	Number = {3},
	Numpages = {22},
	Pages = {57:1--57:22},
	Publisher = {ACM},
	Title = {Factorization Machines with {libFM}},
	Url = {https://dl.acm.org/citation.cfm?doid=2168752.2168771},
	Volume = {3},
	Year = {2012},
	Bdsk-Url-1 = {https://dl.acm.org/citation.cfm?doid=2168752.2168771}}
