%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for Jian Zhang at 2018-02-27 14:38:31 +0800 


%% Saved with string encoding Unicode (UTF-8) 



@article{Srivastava14a,
	Author = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
	Date-Added = {2018-01-31 09:35:27 +0000},
	Date-Modified = {2018-01-31 09:40:13 +0000},
	Journal = {Journal of Machine Learning Research},
	Pages = {1929-1958},
	Title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
	Url = {http://jmlr.org/papers/v15/srivastava14a.html},
	Volume = {15},
	Year = {2014},
	Bdsk-Url-1 = {http://jmlr.org/papers/v15/srivastava14a.html}}

@article{Graves:2013aa,
	Abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
	Author = {Alex Graves},
	Date-Added = {2018-01-31 09:29:40 +0000},
	Date-Modified = {2018-01-31 09:31:58 +0000},
	Eprint = {1308.0850},
	Journal = {CoRR},
	Keywords = {LSTM, Generate Model},
	Month = {08},
	Title = {Generating Sequences With Recurrent Neural Networks},
	Url = {https://arxiv.org/abs/1308.0850},
	Volume = {abs/1308.0850},
	Year = {2013},
	Bdsk-Url-1 = {https://arxiv.org/abs/1308.0850}}

@article{Zaremba:2014aa,
	Abstract = {We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.},
	Author = {Wojciech Zaremba and Ilya Sutskever and Oriol Vinyals},
	Date-Added = {2018-01-31 09:05:49 +0000},
	Date-Modified = {2018-01-31 09:08:38 +0000},
	Eprint = {1409.2329},
	Journal = {CoRR},
	Keywords = {LSTM, Dropout},
	Month = {09},
	Title = {Recurrent Neural Network Regularization},
	Url = {https://arxiv.org/abs/1409.2329},
	Volume = {abs/1409.2329},
	Year = {2014},
	Bdsk-Url-1 = {https://arxiv.org/abs/1409.2329}}

@article{Zeiler:2012aa,
	Abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
	Author = {Matthew D. Zeiler},
	Date-Added = {2018-01-18 14:04:51 +0000},
	Date-Modified = {2018-01-29 06:41:27 +0000},
	Eprint = {1212.5701},
	Journal = {CoRR},
	Keywords = {ADADELTA, Adaptive Learning Rate, SGD},
	Month = {Dec},
	Rating = {0},
	Title = {ADADELTA: An Adaptive Learning Rate Method},
	Url = {https://arxiv.org/abs/1212.5701},
	Volume = {abs/1212.5701},
	Year = {2012},
	Bdsk-Url-1 = {https://arxiv.org/abs/1212.5701}}

@article{Rabiner:HMMtutorial,
	Abstract = {This tutorial provides an overview of the basic theory of hidden Markov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and gives practical details on methods of implementation of the theory along with a description of selected applications of the theory to distinct problems in speech recognition. Results from a number of original sources are combined to provide a single source of acquiring the background required to pursue further this area of research. The author first reviews the theory of discrete Markov chains and shows how the concept of hidden states, where the observation is a probabilistic function of the state, can be used effectively. The theory is illustrated with two simple examples, namely coin-tossing, and the classic balls-in-urns system. Three fundamental problems of HMMs are noted and several practical techniques for solving these problems are given. The various types of HMMs that have been studied, including ergodic as well as left-right models, are described},
	Author = {L. R. Rabiner},
	Date-Modified = {2018-02-27 06:38:28 +0000},
	Doi = {10.1109/5.18626},
	Issn = {0018-9219},
	Journal = {Proceedings of the IEEE},
	Keywords = {Markov processes;speech recognition;balls-in-urns system;coin-tossing;discrete Markov chains;ergodic models;hidden Markov models;hidden states;left-right models;probabilistic function;speech recognition;Distortion;Hidden Markov models;Mathematical model;Multiple signal classification;Signal processing;Speech recognition;Statistical analysis;Stochastic processes;Temperature measurement;Tutorial},
	Month = {Feb},
	Number = {2},
	Pages = {257-286},
	Title = {A tutorial on hidden Markov models and selected applications in speech recognition},
	Volume = {77},
	Year = {1989},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/5.18626}}
