\ifx\mlbook\undefined
    \documentclass[10pt,a4paper]{ctexbook}
    \providecommand{\pathroot}{../..}

    \usepackage[CJKbookmarks,colorlinks,linkcolor=red]{hyperref}
    \usepackage{geometry}
    \usepackage{amsmath}
    \usepackage{minted}

    \geometry{left=3.0cm,right=3.0cm,top=2.5cm,bottom=2.5cm}
    \setmainfont{SimSun}
    \XeTeXlinebreaklocale "zh"
    \XeTeXlinebreakskip = 0pt plus 1pt minus 0.1pt

    \begin{document}
    \setlength{\baselineskip}{20pt}
    \title{微积分}
    \author{Donald Cheung\\jianzhang9102@gmail.com}
    \date{Nov 10, 2017}
    %\maketitle
    \tableofcontents
\fi

\chapter{微积分}
\href{http://dataunion.org/20714.html}{寻找最优参数解：最速下降法，牛顿下降法，阻尼牛顿法，拟牛顿法DFP/BFGS}


\section{梯度下降法}
这一节用来介绍常用的数学优化算法--梯度下降法，以及相关变形。
\href{https://arxiv.org/abs/1609.04747}{An overview of gradient descent optimization algorithms}

\href{http://ruder.io/optimizing-gradient-descent}{An overview of gradient descent optimization algorithms}

\href{https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants}{Introduction to Gradient Descent Algorithm (along with variants) in Machine Learning}

\begin{minted}{python}
w = 0
for i in range(0, n):
    w = w - 0.1 * f'(w)
\end{minted}

\subsection{Gradient descent variants}
\subsubsection{Batch gradient descent}
\subsubsection{Stochastic gradient descent}
\subsubsection{Mini-batch gradient descent}

\subsection{Gradient descent optimization algorithms}
\subsubsection{Momentum}
\subsubsection{Nesterov accelerated gradient}
\subsubsection{Adagrad}
\subsubsection{Adadelta}
\subsubsection{RMSprop}
\subsubsection{Adam}
\subsubsection{AdaMax}
\subsubsection{Nadam}
\subsubsection{Visualization of algorithms}
\subsubsection{Which optimizer to choose?}

\subsection{Parallelizing and distributing SGD}
\subsubsection{Hogwild!}
\subsubsection{Downpour SGD}
\subsubsection{Delay-tolerant Algorithms for SGD}
\subsubsection{TensorFlow}
\subsubsection{Elastic Averaging SGD}

\subsection{Additional strategies for optimizing SGD}
\subsubsection{Shuffling and Curriculum Learning}
\subsubsection{Batch normalization}
\subsubsection{Early Stopping}
\subsubsection{Gradient noise}


\section{牛顿法}
\subsection{拟牛顿条件}

\section{拟牛顿法（Quasi-Newton）}
\subsection{DFP 算法}
\subsection{BFGS算法}
\subsection{L-BFGS 算法}

\section{共轭梯度法(Conjugate Gradient)}


 
学习资料
\begin{enumerate}
\item《最优化理论与方法》 袁亚湘 孙文瑜
\item http://blog.csdn.net/nocml/article/details/8287466
\item Updating Quasi-Newton Matrices with Limited Storage , Jorge Nocedal
\item Nonlinear Programming, second edition, Dimitri P. Bertsekas
\item《广告数据上的大规模机器学习》  夏粉
\end{enumerate}

\ifx\mlbook\undefined
    \end{document}
\fi
