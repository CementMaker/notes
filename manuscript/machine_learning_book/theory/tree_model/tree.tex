\ifx\mlbook\undefined
    \documentclass[10pt,a4paper]{ctexbook}
    \providecommand{\pathroot}{../..}

    \usepackage[CJKbookmarks,colorlinks,linkcolor=red]{hyperref}
    \usepackage{geometry}
    \usepackage{amsmath}

    \usepackage{float}
    \usepackage[table, x11names]{xcolor}
    \usepackage{array, booktabs, boldline}
    \usepackage{cellspace}

    \geometry{left=3.0cm,right=3.0cm,top=2.5cm,bottom=2.5cm}
    \setmainfont{SimSun}
    \XeTeXlinebreaklocale "zh"
    \XeTeXlinebreakskip = 0pt plus 1pt minus 0.1pt

    \begin{document}
    \setlength{\baselineskip}{20pt}
    \title{决策树}
    \author{Donald Cheung\\jianzhang9102@gmail.com}
    \date{Nov 8, 2017}
    %\maketitle
    \tableofcontents
\fi

\chapter{决策树}


信息量度量的是一个具体事件发生了所带来的信息，而熵则是在结果出来之前对可能产生的信息量的期望--考虑该随机变量的所有可能取值，即所有可能发生事件所带来的信息量的期望。对于某一个变量X其信息熵为：
\[
信息熵= -\sum\limits_{i}{p_{i}log(p_{i})}, 其中：p_{i}为X取变量i时的概率。
\]

对于只有$n$个取值的随机变量，信息熵为：
\[
信息熵=-\sum\limits_{i}{p_{i}log(p_{i})}
\]
其中：
\[
\sum\limits_{i}{p_{i}}=1
\]
由凸优化理论可以证明当$p_{i}=\frac{1}{n}$时，信息熵最大，即所有元素的发生的概率相同，信息熵最大。


对于任意个具有$m$个元素的集合，对于元素$i$的概率为$p_{i}$，则集合的信息熵为：
\[
H(X)=-\sum{p_{i}log(p_{i})}
\]
由上一页可知，当每个元素出现的概率相等时信息熵越大，说明集合中的每类元素个数越趋于相同时（此时集合越混乱），信息熵越大，当集合中元素的个数差距越大（此时集合越纯净），信息熵越小。


前面仅仅由一个变量的情况，当我们的变量有两个，并且它们的联合概率分布如下表所示：
\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
        & $y_{1}$ & $y_{2}$ & $\cdots$ & $y_{j}$ & $\cdots$ & $y_{m}$ \\
\hline
$x_{1}$ & $p(1,1)$ & $p(1,2)$ & $\cdots$ & $p(1,j)$ & $\cdots$ & $p(1,m)$ \\
\hline
$x_{2}$ & $p(2,1)$ & $p(2,2)$ & $\cdots$ & $p(2,j)$ & $\cdots$ & $p(2,m)$ \\
\hline
$\vdots$ & & & & & & \\
\hline
$x_{i}$ & $p(i,1)$ & $p(i,2)$ & $\cdots$ & $p(i,j)$ & $\cdots$ & $p(i,m)$ \\
\hline
$\vdots$ & & & & & & \\
\hline
$x_{n}$ & $p(n,1)$ & $p(n,2)$ & $\cdots$ & $p(n,j)$ & $\cdots$ & $p(n,m)$ \\
\hline
\end{tabular}%
\caption{复合熵}
\label{tab:complex-entropy}
\end{table}


条件熵：
\[
H(x|y_{j})=-\sum\limits_{i=1}^{n}{p(x_{i}|y_{j})\log{p(x_{i}|y_{j})}}
\]


\textbf{决策树--最小二乘回归树算法}
\begin{enumerate}
\item 选择最优切片变量$j$与切片点$s$，求解
\[
\min\limits_{j,s}\left[\min\limits_{c_1}{\sum\limits_{x_i \in R_{1}{(j,s)}}{(y_{i}-c_{1})^2}}+\min\limits_{c_2}{\sum\limits_{x_{i} \in R_{2}{(j,s)}}{(y_{i}-c_{2})^2}}\right]
\]
遍历变量$j$，对于固定的切分变量$j$扫描切分点$s$，选择使上式达到最小值的对$(j,s)$

\item 用选定的对$(j,s)$划分区域并决定相应的输出值：
\[
\begin{aligned}
R_{1}{(j,s)}=\{x|x^{(j)} \le s \}, R_{2}{(j,s)}=\{x|x^{(j)} > s \}\\
\hat{c_{m}}=\frac{1}{N_{m}}\sum\limits_{x_{i} \in R_{m}{(j,s)}}{y_{i}}, \quad x \in R_{m}, \quad m=1,2
\end{aligned}
\]


\item 继续对两个子区域调用1，2，直至满足条件。
\item 将输入空间划分成M个区域，并生成决策树：
\[
f(x)=\sum\limits_{m=1}^{M}{\hat{c_m}I(x \in R_{m})}
\]

\textbf{决策树--后剪枝}

决策树的剪枝往往通过极小化决策树的损失函数或者代价函数来实现。设树T的节点的个数为$|T|$，$t$是$T$的叶节点，该叶节点有$N_{t}$个样本点，其中$K$类的样本点有$N_{tk}$个，$H_{t}{(T)}$为叶节点$t$上的经验熵，$\alpha \ge 0$为参数，则损失函数定义为
\[
C_{\alpha}{(T)}=\sum\limits_{t=1}^{|T|}{N_{t}H_{t}{(T)}+\alpha|T|}
\]
其中经验熵为：
\[
H_{t}{(T)}=-\sum\limits_{k}{\frac{N_{tk}}{N_{t}}\log \frac{N_{tk}}{N_{t}}}
\]
令$C(T)=\sum\limits_{t=1}^{|T|}{N_{t}H_{t}{(T)}}=-\sum\limits_{t=1}^{|T|}\sum\limits_{k=1}^{K}{N_{tk}\log \frac{N_{tk}}{N_{t}}}$， 这时候有
\[
C_{\alpha}{(T)}=C(T)+\alpha |T|
\]






\end{enumerate}




\ifx\mlbook\undefined
    \end{document}
\fi
